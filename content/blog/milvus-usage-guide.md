+++
title = "Milvus å‘é‡æ•°æ®åº“ä½¿ç”¨æŒ‡å—"
date = 2025-10-09T12:00:00Z
description = "æ·±å…¥äº†è§£ Milvus å‘é‡æ•°æ®åº“çš„æ¶æ„ã€å®‰è£…éƒ¨ç½²å’Œå®æˆ˜åº”ç”¨"

[taxonomies]
tags = ["Milvus", "å‘é‡æ•°æ®åº“", "AI", "æœºå™¨å­¦ä¹ ", "ç›¸ä¼¼åº¦æœç´¢"]
categories = ["æ•°æ®åº“", "AI"]
+++

## Milvus ç®€ä»‹

Milvus æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œä¸“ä¸ºæµ·é‡å‘é‡æ•°æ®çš„å­˜å‚¨ã€ç´¢å¼•å’ŒæŸ¥è¯¢è€Œè®¾è®¡ã€‚å®ƒåœ¨ AI åº”ç”¨ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥ä¸‹åœºæ™¯ï¼š

- **ç›¸ä¼¼åº¦æœç´¢**ï¼šå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘çš„ç›¸ä¼¼å†…å®¹æ£€ç´¢
- **æ¨èç³»ç»Ÿ**ï¼šåŸºäºç”¨æˆ·è¡Œä¸ºå’Œå†…å®¹ç‰¹å¾çš„ä¸ªæ€§åŒ–æ¨è
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šè¯­ä¹‰æœç´¢ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬åˆ†ç±»
- **å¼‚å¸¸æ£€æµ‹**ï¼šç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ç­‰åœºæ™¯

### æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**ï¼šæ”¯æŒåäº¿çº§å‘é‡çš„æ¯«ç§’çº§æŸ¥è¯¢
- ğŸ“Š **å¤šç§ç´¢å¼•ç±»å‹**ï¼šFLATã€IVF_FLATã€IVF_SQ8ã€IVF_PQã€HNSWã€ANNOY ç­‰
- ğŸ”„ **æ··åˆæŸ¥è¯¢**ï¼šæ”¯æŒå‘é‡æ£€ç´¢ä¸æ ‡é‡è¿‡æ»¤çš„ç»„åˆæŸ¥è¯¢
- ğŸŒ **äº‘åŸç”Ÿæ¶æ„**ï¼šå­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
- ğŸ›¡ï¸ **é«˜å¯ç”¨æ€§**ï¼šæ”¯æŒæ•°æ®å¤‡ä»½ã€æ•…éšœæ¢å¤
- ğŸ”Œ **å¤šè¯­è¨€ SDK**ï¼šPythonã€Javaã€Goã€Node.js ç­‰

## Milvus æ ¸å¿ƒæ¦‚å¿µ

### æ¶æ„ç»„ä»¶è¯´æ˜

#### 1. æ¥å…¥å±‚ï¼ˆAccess Layerï¼‰
- **Proxy**ï¼šæ— çŠ¶æ€çš„ä»£ç†æœåŠ¡ï¼Œè´Ÿè´£è¯·æ±‚è·¯ç”±ã€è´Ÿè½½å‡è¡¡ã€ç»“æœèšåˆ

#### 2. åè°ƒæœåŠ¡å±‚ï¼ˆCoordinator Serviceï¼‰
- **Root Coordinator**ï¼šç®¡ç†é›†ç¾¤æ‹“æ‰‘ã€DDL æ“ä½œï¼ˆåˆ›å»º/åˆ é™¤ Collectionï¼‰
- **Query Coordinator**ï¼šç®¡ç†æŸ¥è¯¢èŠ‚ç‚¹çš„æ‹“æ‰‘å’Œè´Ÿè½½å‡è¡¡
- **Data Coordinator**ï¼šç®¡ç†æ•°æ®èŠ‚ç‚¹ã€æ•°æ®æ®µçš„åˆ†é…å’ŒæŒä¹…åŒ–
- **Index Coordinator**ï¼šç®¡ç†ç´¢å¼•æ„å»ºä»»åŠ¡çš„è°ƒåº¦

#### 3. æ‰§è¡Œå±‚ï¼ˆWorker Nodeï¼‰
- **Query Node**ï¼šæ‰§è¡Œå‘é‡æ£€ç´¢å’Œæ ‡é‡æŸ¥è¯¢
- **Data Node**ï¼šè´Ÿè´£æ•°æ®çš„æµå¼å†™å…¥å’ŒæŒä¹…åŒ–
- **Index Node**ï¼šè´Ÿè´£ç´¢å¼•çš„æ„å»º

#### 4. å­˜å‚¨å±‚ï¼ˆStorageï¼‰
- **Meta Store**ï¼šå­˜å‚¨å…ƒæ•°æ®ï¼ˆetcdï¼‰
- **Message Queue**ï¼šæµå¼æ•°æ®ä¼ è¾“ï¼ˆPulsar/Kafkaï¼‰
- **Object Storage**ï¼šæŒä¹…åŒ–å­˜å‚¨å‘é‡æ•°æ®å’Œç´¢å¼•ï¼ˆMinIO/S3ï¼‰

## çŸ¥è¯†åº“çš„ç»„ç»‡æ–¹å¼

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåˆç†ç»„ç»‡çŸ¥è¯†åº“ç»“æ„å¯¹äºé«˜æ•ˆæ£€ç´¢è‡³å…³é‡è¦ã€‚ä»¥ä¸‹æ˜¯å‡ ç§å¸¸è§çš„ç»„ç»‡æ–¹å¼ï¼š

### 1. å• Collection æ¨¡å¼

é€‚ç”¨äºå°è§„æ¨¡ã€å•ä¸€ç±»å‹çš„çŸ¥è¯†åº“ã€‚

```mermaid
graph LR
    A[çŸ¥è¯†åº“] --> B[Collection: documents]
    B --> C[æ–‡æ¡£1]
    B --> D[æ–‡æ¡£2]
    B --> E[æ–‡æ¡£3]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e1ffe1
    style D fill:#e1ffe1
    style E fill:#e1ffe1
```

**ä¼˜ç‚¹**ï¼šç»“æ„ç®€å•ï¼Œæ˜“äºç®¡ç†
**ç¼ºç‚¹**ï¼šä¸é€‚åˆå¤šç§Ÿæˆ·æˆ–å¤šç±»å‹æ–‡æ¡£

### 2. å¤š Collection æ¨¡å¼

æŒ‰æ–‡æ¡£ç±»å‹æˆ–ä¸šåŠ¡é¢†åŸŸåˆ’åˆ†ä¸åŒçš„ Collectionã€‚

```mermaid
graph TB
    A[çŸ¥è¯†åº“ç³»ç»Ÿ] --> B[Collection: tech_docs<br/>æŠ€æœ¯æ–‡æ¡£]
    A --> C[Collection: product_docs<br/>äº§å“æ–‡æ¡£]
    A --> D[Collection: customer_qa<br/>å®¢æˆ·é—®ç­”]

    B --> B1[APIæ–‡æ¡£]
    B --> B2[æ¶æ„è®¾è®¡]
    C --> C1[äº§å“æ‰‹å†Œ]
    C --> C2[ä½¿ç”¨æŒ‡å—]
    D --> D1[å¸¸è§é—®é¢˜]
    D --> D2[å·¥å•è®°å½•]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#fff4e1
    style D fill:#fff4e1
    style B1 fill:#e1ffe1
    style B2 fill:#e1ffe1
    style C1 fill:#e1ffe1
    style C2 fill:#e1ffe1
    style D1 fill:#e1ffe1
    style D2 fill:#e1ffe1
```

**ä¼˜ç‚¹**ï¼šé€»è¾‘æ¸…æ™°ï¼Œä¾¿äºæƒé™æ§åˆ¶
**ç¼ºç‚¹**ï¼šè·¨ Collection æŸ¥è¯¢è¾ƒå¤æ‚

### 3. åˆ†åŒºï¼ˆPartitionï¼‰æ¨¡å¼

åœ¨å•ä¸ª Collection å†…ä½¿ç”¨åˆ†åŒºè¿›è¡Œé€»è¾‘éš”ç¦»ã€‚

```mermaid
graph TB
    A[Collection: knowledge_base] --> B[Partition: 2024_Q1]
    A --> C[Partition: 2024_Q2]
    A --> D[Partition: 2024_Q3]
    A --> E[Partition: archived]

    B --> B1[1æœˆæ–‡æ¡£]
    B --> B2[2æœˆæ–‡æ¡£]
    B --> B3[3æœˆæ–‡æ¡£]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#fff4e1
    style D fill:#fff4e1
    style E fill:#fff4e1
    style B1 fill:#e1ffe1
    style B2 fill:#e1ffe1
    style B3 fill:#e1ffe1
```

**ä¼˜ç‚¹**ï¼šç»Ÿä¸€ç®¡ç†ï¼Œæ”¯æŒæŒ‰åˆ†åŒºæŸ¥è¯¢
**ç¼ºç‚¹**ï¼šåˆ†åŒºæ•°é‡æœ‰é™åˆ¶ï¼ˆå»ºè®®ä¸è¶…è¿‡ 4096 ä¸ªï¼‰

### 4. æ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰

ç»“åˆ Collection å’Œ Partitionï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å¤šç§Ÿæˆ·åœºæ™¯ã€‚

```mermaid
graph TB
    A[ä¼ä¸šçŸ¥è¯†åº“] --> B[Collection: tenant_A]
    A --> C[Collection: tenant_B]

    B --> B1[Partition: public]
    B --> B2[Partition: private]
    B --> B3[Partition: archived]

    C --> C1[Partition: dept_sales]
    C --> C2[Partition: dept_tech]
    C --> C3[Partition: dept_hr]

    B1 --> B1A[å…¬å¼€æ–‡æ¡£]
    B2 --> B2A[å†…éƒ¨æ–‡æ¡£]
    C1 --> C1A[é”€å”®èµ„æ–™]
    C2 --> C2A[æŠ€æœ¯æ–‡æ¡£]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#fff4e1
    style B1 fill:#ffe1f5
    style B2 fill:#ffe1f5
    style B3 fill:#ffe1f5
    style C1 fill:#ffe1f5
    style C2 fill:#ffe1f5
    style C3 fill:#ffe1f5
    style B1A fill:#e1ffe1
    style B2A fill:#e1ffe1
    style C1A fill:#e1ffe1
    style C2A fill:#e1ffe1
```

**ä¼˜ç‚¹**ï¼šçµæ´»æ€§é«˜ï¼Œæ”¯æŒå¤æ‚åœºæ™¯
**ç¼ºç‚¹**ï¼šç®¡ç†å¤æ‚åº¦è¾ƒé«˜

## åº”ç”¨å±‚æ–‡æ¡£å¤„ç†æµç¨‹

### æ–‡æ¡£å…¥åº“å®Œæ•´æµç¨‹

```mermaid
graph TB
    A[åŸå§‹æ–‡æ¡£] --> B[æ–‡æ¡£é¢„å¤„ç†]
    B --> C[æ–‡æœ¬åˆ†å—<br/>Chunking]
    C --> D[å‘é‡åŒ–<br/>Embedding]
    D --> E[å…ƒæ•°æ®æå–]
    E --> F[å­˜å…¥ Milvus]
    F --> G[åˆ›å»ºç´¢å¼•]
    G --> H[çŸ¥è¯†åº“å°±ç»ª]

    B --> B1[æ¸…æ´—HTMLæ ‡ç­¾]
    B --> B2[æ ¼å¼è½¬æ¢]
    B --> B3[å»é‡å¤„ç†]

    C --> C1[å›ºå®šé•¿åº¦åˆ†å—]
    C --> C2[è¯­ä¹‰åˆ†å—]
    C --> C3[é‡å çª—å£]

    D --> D1[é€‰æ‹©æ¨¡å‹]
    D --> D2[æ‰¹é‡ç¼–ç ]

    E --> E1[æ ‡é¢˜/ä½œè€…]
    E --> E2[æ—¶é—´æˆ³]
    E --> E3[åˆ†ç±»æ ‡ç­¾]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#fff4e1
    style D fill:#fff4e1
    style E fill:#fff4e1
    style F fill:#ffe1f5
    style G fill:#ffe1f5
    style H fill:#e1ffe1
```

### 1. æ–‡æ¡£é¢„å¤„ç†

```python
import re
from typing import Dict, List
import hashlib

class DocumentPreprocessor:
    """æ–‡æ¡£é¢„å¤„ç†å™¨"""

    def clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬"""
        # ç§»é™¤ HTML æ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()ï¼ˆï¼‰ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]', '', text)
        return text.strip()

    def calculate_hash(self, text: str) -> str:
        """è®¡ç®—æ–‡æ¡£å“ˆå¸Œç”¨äºå»é‡"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def extract_metadata(self, doc: Dict) -> Dict:
        """æå–å…ƒæ•°æ®"""
        return {
            'title': doc.get('title', ''),
            'author': doc.get('author', ''),
            'source': doc.get('source', ''),
            'category': doc.get('category', ''),
            'created_at': doc.get('created_at', ''),
            'tags': doc.get('tags', [])
        }
```

### 2. æ–‡æœ¬åˆ†å—ç­–ç•¥

```python
from typing import List

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def fixed_size_chunking(self, text: str) -> List[str]:
        """å›ºå®šé•¿åº¦åˆ†å—"""
        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            end = start + self.chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - self.overlap

        return chunks

    def semantic_chunking(self, text: str) -> List[str]:
        """è¯­ä¹‰åˆ†å—ï¼ˆæŒ‰æ®µè½ï¼‰"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def sentence_chunking(self, text: str) -> List[str]:
        """å¥å­çº§åˆ†å—"""
        import re
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­è‹±æ–‡ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
        chunks = []
        current_chunk = ""

        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue

            if len(current_chunk) + len(sent) <= self.chunk_size:
                current_chunk += sent + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sent + "ã€‚"

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks
```

### 3. å‘é‡åŒ–å¤„ç†

```python
from sentence_transformers import SentenceTransformer
from typing import List
import numpy as np

class EmbeddingGenerator:
    """å‘é‡ç”Ÿæˆå™¨"""

    def __init__(self, model_name: str = 'paraphrase-multilingual-MiniLM-L12-v2'):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()

    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True  # å½’ä¸€åŒ–å‘é‡
        )
        return embeddings

    def encode_single(self, text: str) -> np.ndarray:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        return self.model.encode([text], normalize_embeddings=True)[0]
```

### 4. å®Œæ•´çš„æ–‡æ¡£å…¥åº“ç³»ç»Ÿ

```python
from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType
from typing import List, Dict
import uuid

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""

    def __init__(self, collection_name: str = "knowledge_base"):
        # è¿æ¥ Milvus
        connections.connect(host="localhost", port="19530")

        self.collection_name = collection_name
        self.preprocessor = DocumentPreprocessor()
        self.chunker = TextChunker(chunk_size=500, overlap=50)
        self.embedder = EmbeddingGenerator()

        # åˆ›å»ºæˆ–è·å– Collection
        self.collection = self._create_collection()

    def _create_collection(self) -> Collection:
        """åˆ›å»º Collection"""
        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=64),
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=200),
            FieldSchema(name="created_at", dtype=DataType.INT64),
        ]

        schema = CollectionSchema(fields=fields, description="ä¼ä¸šçŸ¥è¯†åº“")

        try:
            collection = Collection(name=self.collection_name, schema=schema)
        except:
            collection = Collection(name=self.collection_name)

        return collection

    def ingest_document(self, document: Dict) -> int:
        """
        å…¥åº“å•ä¸ªæ–‡æ¡£

        Args:
            document: æ–‡æ¡£å­—å…¸ï¼ŒåŒ…å« 'content', 'title', 'author' ç­‰å­—æ®µ

        Returns:
            æ’å…¥çš„åˆ†å—æ•°é‡
        """
        # 1. é¢„å¤„ç†
        text = self.preprocessor.clean_text(document['content'])
        doc_hash = self.preprocessor.calculate_hash(text)
        metadata = self.preprocessor.extract_metadata(document)

        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.chunker.semantic_chunking(text)

        # 3. ç”Ÿæˆå‘é‡
        embeddings = self.embedder.encode_texts(chunks)

        # 4. å‡†å¤‡æ•°æ®
        ids = [str(uuid.uuid4()) for _ in range(len(chunks))]
        doc_ids = [doc_hash] * len(chunks)
        chunk_indices = list(range(len(chunks)))
        titles = [metadata['title']] * len(chunks)
        authors = [metadata.get('author', '')] * len(chunks)
        categories = [metadata.get('category', '')] * len(chunks)
        sources = [metadata.get('source', '')] * len(chunks)
        timestamps = [int(document.get('timestamp', 0))] * len(chunks)

        # 5. æ’å…¥æ•°æ®
        entities = [
            ids,
            embeddings.tolist(),
            chunks,
            doc_ids,
            chunk_indices,
            titles,
            authors,
            categories,
            sources,
            timestamps
        ]

        self.collection.insert(entities)
        self.collection.flush()

        return len(chunks)

    def batch_ingest_documents(self, documents: List[Dict]) -> Dict[str, int]:
        """æ‰¹é‡å…¥åº“æ–‡æ¡£"""
        stats = {'total': len(documents), 'chunks': 0, 'failed': 0}

        for doc in documents:
            try:
                chunks_count = self.ingest_document(doc)
                stats['chunks'] += chunks_count
            except Exception as e:
                print(f"æ–‡æ¡£å…¥åº“å¤±è´¥: {doc.get('title', 'Unknown')}, é”™è¯¯: {e}")
                stats['failed'] += 1

        return stats

    def create_index(self):
        """åˆ›å»ºç´¢å¼•"""
        index_params = {
            "metric_type": "COSINE",
            "index_type": "HNSW",
            "params": {"M": 16, "efConstruction": 200}
        }

        self.collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        print("ç´¢å¼•åˆ›å»ºæˆåŠŸ")

    def load_collection(self):
        """åŠ è½½ Collection åˆ°å†…å­˜"""
        self.collection.load()
        print("Collection å·²åŠ è½½")

    def search(self, query: str, top_k: int = 5, filters: str = None) -> List[Dict]:
        """
        æœç´¢çŸ¥è¯†åº“

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: è¿‡æ»¤è¡¨è¾¾å¼ï¼Œå¦‚ "category == 'æŠ€æœ¯æ–‡æ¡£'"

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedder.encode_single(query)

        # æœç´¢å‚æ•°
        search_params = {"metric_type": "COSINE", "params": {"ef": 100}}

        # æ‰§è¡Œæœç´¢
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=filters,
            output_fields=["text", "title", "author", "category", "source", "chunk_index"]
        )

        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for hit in results[0]:
            formatted_results.append({
                'score': hit.distance,
                'text': hit.entity.get('text'),
                'title': hit.entity.get('title'),
                'author': hit.entity.get('author'),
                'category': hit.entity.get('category'),
                'source': hit.entity.get('source'),
                'chunk_index': hit.entity.get('chunk_index')
            })

        return formatted_results
```

### 5. ä½¿ç”¨ç¤ºä¾‹

```python
# åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
kb_manager = KnowledgeBaseManager(collection_name="company_kb")

# å‡†å¤‡æ–‡æ¡£æ•°æ®
documents = [
    {
        'content': """
        Milvus æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œä¸“ä¸ºæµ·é‡å‘é‡æ•°æ®çš„å­˜å‚¨ã€ç´¢å¼•å’ŒæŸ¥è¯¢è€Œè®¾è®¡ã€‚
        å®ƒæ”¯æŒå¤šç§ç´¢å¼•ç±»å‹ï¼ŒåŒ…æ‹¬ FLATã€IVF_FLATã€HNSW ç­‰ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„ç´¢å¼•ã€‚
        Milvus é‡‡ç”¨äº‘åŸç”Ÿæ¶æ„ï¼Œæ”¯æŒå­˜å‚¨è®¡ç®—åˆ†ç¦»ï¼Œå¯ä»¥æ°´å¹³æ‰©å±•ä»¥åº”å¯¹å¤§è§„æ¨¡æ•°æ®ã€‚
        """,
        'title': 'Milvus å‘é‡æ•°æ®åº“ä»‹ç»',
        'author': 'å¼ ä¸‰',
        'category': 'æŠ€æœ¯æ–‡æ¡£',
        'source': 'internal_wiki',
        'timestamp': 1696838400
    },
    {
        'content': """
        åœ¨æ„å»º RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿæ—¶ï¼Œå‘é‡æ•°æ®åº“æ‰®æ¼”ç€å…³é”®è§’è‰²ã€‚
        å®ƒè´Ÿè´£å­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºï¼Œå¹¶åœ¨ç”¨æˆ·æŸ¥è¯¢æ—¶å¿«é€Ÿæ£€ç´¢ç›¸å…³å†…å®¹ã€‚
        é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ£€ç´¢ï¼ŒRAG ç³»ç»Ÿå¯ä»¥æä¾›å‡†ç¡®ä¸”å…·æœ‰ä¸Šä¸‹æ–‡çš„å›ç­”ã€‚
        """,
        'title': 'RAG ç³»ç»Ÿæ¶æ„è®¾è®¡',
        'author': 'æå››',
        'category': 'æ¶æ„è®¾è®¡',
        'source': 'tech_blog',
        'timestamp': 1696924800
    }
]

# æ‰¹é‡å…¥åº“
stats = kb_manager.batch_ingest_documents(documents)
print(f"å…¥åº“ç»Ÿè®¡: {stats}")

# åˆ›å»ºç´¢å¼•
kb_manager.create_index()

# åŠ è½½åˆ°å†…å­˜
kb_manager.load_collection()

# æœç´¢
results = kb_manager.search(
    query="å¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Ÿ",
    top_k=3,
    filters="category == 'æŠ€æœ¯æ–‡æ¡£'"
)

# æ‰“å°ç»“æœ
for i, result in enumerate(results, 1):
    print(f"\nç»“æœ {i}:")
    print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
    print(f"æ ‡é¢˜: {result['title']}")
    print(f"å†…å®¹: {result['text'][:100]}...")
```

### 6. é«˜çº§ç‰¹æ€§ï¼šå¢é‡æ›´æ–°

```python
class IncrementalUpdater:
    """å¢é‡æ›´æ–°ç®¡ç†å™¨"""

    def __init__(self, kb_manager: KnowledgeBaseManager):
        self.kb_manager = kb_manager

    def update_document(self, doc_id: str, new_content: Dict):
        """æ›´æ–°æ–‡æ¡£"""
        # 1. åˆ é™¤æ—§æ–‡æ¡£çš„æ‰€æœ‰åˆ†å—
        expr = f"doc_id == '{doc_id}'"
        self.kb_manager.collection.delete(expr)

        # 2. æ’å…¥æ–°æ–‡æ¡£
        self.kb_manager.ingest_document(new_content)

        print(f"æ–‡æ¡£ {doc_id} æ›´æ–°æˆåŠŸ")

    def delete_document(self, doc_id: str):
        """åˆ é™¤æ–‡æ¡£"""
        expr = f"doc_id == '{doc_id}'"
        self.kb_manager.collection.delete(expr)
        print(f"æ–‡æ¡£ {doc_id} åˆ é™¤æˆåŠŸ")
```

## å®‰è£…éƒ¨ç½²

### æ–¹å¼ä¸€ï¼šDocker Composeï¼ˆæ¨èç”¨äºå¼€å‘æµ‹è¯•ï¼‰

```bash
# ä¸‹è½½ docker-compose.yml
wget https://github.com/milvus-io/milvus/releases/download/v2.3.0/milvus-standalone-docker-compose.yml -O docker-compose.yml

# å¯åŠ¨ Milvus
docker-compose up -d

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps
```

### æ–¹å¼äºŒï¼šKubernetesï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰

```bash
# æ·»åŠ  Milvus Helm ä»“åº“
helm repo add milvus https://milvus-io.github.io/milvus-helm/
helm repo update

# å®‰è£… Milvus
helm install milvus milvus/milvus --set cluster.enabled=true

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods
```

### æ–¹å¼ä¸‰ï¼šäºŒè¿›åˆ¶å®‰è£…

```bash
# ä¸‹è½½ Milvus äºŒè¿›åˆ¶æ–‡ä»¶
wget https://github.com/milvus-io/milvus/releases/download/v2.3.0/milvus-standalone-linux-amd64.tar.gz

# è§£å‹å¹¶å¯åŠ¨
tar -xzf milvus-standalone-linux-amd64.tar.gz
cd milvus
./bin/milvus run standalone
```

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Python SDK

```bash
pip install pymilvus
```

### 2. è¿æ¥åˆ° Milvus

```python
from pymilvus import connections, utility

# è¿æ¥åˆ° Milvus
connections.connect(
    alias="default",
    host="localhost",
    port="19530"
)

# æ£€æŸ¥è¿æ¥
print(f"Milvus ç‰ˆæœ¬: {utility.get_server_version()}")
```

### 3. åˆ›å»º Collection

```python
from pymilvus import CollectionSchema, FieldSchema, DataType, Collection

# å®šä¹‰å­—æ®µ
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128),
    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=200),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=50)
]

# åˆ›å»º Schema
schema = CollectionSchema(
    fields=fields,
    description="æ–‡æ¡£å‘é‡åº“"
)

# åˆ›å»º Collection
collection = Collection(
    name="documents",
    schema=schema
)

print(f"Collection åˆ›å»ºæˆåŠŸ: {collection.name}")
```

### 4. æ’å…¥æ•°æ®

```python
import random

# å‡†å¤‡æ•°æ®
num_entities = 1000
embeddings = [[random.random() for _ in range(128)] for _ in range(num_entities)]
titles = [f"æ–‡æ¡£_{i}" for i in range(num_entities)]
categories = [random.choice(["æŠ€æœ¯", "äº§å“", "å¸‚åœº"]) for _ in range(num_entities)]

# æ’å…¥æ•°æ®
entities = [
    embeddings,
    titles,
    categories
]

insert_result = collection.insert(entities)
print(f"æ’å…¥æ•°æ®æ•°é‡: {len(insert_result.primary_keys)}")

# åˆ·æ–°æ•°æ®ï¼ˆç¡®ä¿æ•°æ®æŒä¹…åŒ–ï¼‰
collection.flush()
```

### 5. åˆ›å»ºç´¢å¼•

```python
# å®šä¹‰ç´¢å¼•å‚æ•°
index_params = {
    "metric_type": "L2",  # è·ç¦»åº¦é‡ï¼šL2ï¼ˆæ¬§æ°è·ç¦»ï¼‰æˆ– IPï¼ˆå†…ç§¯ï¼‰
    "index_type": "IVF_FLAT",  # ç´¢å¼•ç±»å‹
    "params": {"nlist": 128}  # ç´¢å¼•å‚æ•°
}

# åˆ›å»ºç´¢å¼•
collection.create_index(
    field_name="embedding",
    index_params=index_params
)

print("ç´¢å¼•åˆ›å»ºæˆåŠŸ")
```

### 6. åŠ è½½ Collection

```python
# å°† Collection åŠ è½½åˆ°å†…å­˜
collection.load()
print("Collection å·²åŠ è½½åˆ°å†…å­˜")
```

### 7. æ‰§è¡Œå‘é‡æœç´¢

```python
# å‡†å¤‡æŸ¥è¯¢å‘é‡
search_vectors = [[random.random() for _ in range(128)] for _ in range(5)]

# å®šä¹‰æœç´¢å‚æ•°
search_params = {
    "metric_type": "L2",
    "params": {"nprobe": 10}  # æœç´¢çš„èšç±»æ•°é‡
}

# æ‰§è¡Œæœç´¢
results = collection.search(
    data=search_vectors,
    anns_field="embedding",
    param=search_params,
    limit=10,  # è¿”å› Top-K ç»“æœ
    output_fields=["title", "category"]  # è¿”å›çš„å­—æ®µ
)

# æ‰“å°ç»“æœ
for i, result in enumerate(results):
    print(f"\næŸ¥è¯¢ {i + 1} çš„ç»“æœ:")
    for hit in result:
        print(f"  ID: {hit.id}, è·ç¦»: {hit.distance:.4f}, "
              f"æ ‡é¢˜: {hit.entity.get('title')}, "
              f"åˆ†ç±»: {hit.entity.get('category')}")
```

### 8. æ··åˆæŸ¥è¯¢ï¼ˆå‘é‡æœç´¢ + æ ‡é‡è¿‡æ»¤ï¼‰

```python
# ä½¿ç”¨è¡¨è¾¾å¼è¿‡æ»¤
expr = "category == 'æŠ€æœ¯'"

results = collection.search(
    data=search_vectors,
    anns_field="embedding",
    param=search_params,
    limit=10,
    expr=expr,  # è¿‡æ»¤è¡¨è¾¾å¼
    output_fields=["title", "category"]
)

print("\nè¿‡æ»¤åçš„æœç´¢ç»“æœ:")
for i, result in enumerate(results):
    print(f"æŸ¥è¯¢ {i + 1} æ‰¾åˆ° {len(result)} ä¸ªç»“æœ")
```

## é«˜çº§ç‰¹æ€§

### 1. åˆ†åŒºç®¡ç†

```python
# åˆ›å»ºåˆ†åŒº
collection.create_partition("partition_2024")
collection.create_partition("partition_2025")

# å‘ç‰¹å®šåˆ†åŒºæ’å…¥æ•°æ®
partition = collection.partition("partition_2024")
partition.insert(entities)

# åœ¨ç‰¹å®šåˆ†åŒºä¸­æœç´¢
results = collection.search(
    data=search_vectors,
    anns_field="embedding",
    param=search_params,
    limit=10,
    partition_names=["partition_2024"]
)
```

### 2. ç´¢å¼•ç±»å‹é€‰æ‹©

| ç´¢å¼•ç±»å‹ | é€‚ç”¨åœºæ™¯ | å†…å­˜å ç”¨ | æŸ¥è¯¢é€Ÿåº¦ |
|---------|---------|---------|---------|
| FLAT | å°æ•°æ®é›†ï¼Œè¿½æ±‚ç²¾ç¡®åº¦ | é«˜ | æ…¢ |
| IVF_FLAT | ä¸­ç­‰æ•°æ®é›†ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦ | ä¸­ | ä¸­ |
| IVF_SQ8 | å¤§æ•°æ®é›†ï¼ŒèŠ‚çœå†…å­˜ | ä½ | ä¸­ |
| IVF_PQ | è¶…å¤§æ•°æ®é›†ï¼Œæè‡´å‹ç¼© | æä½ | å¿« |
| HNSW | é«˜ç²¾åº¦è¦æ±‚ï¼Œå†…å­˜å……è¶³ | é«˜ | æå¿« |
| ANNOY | é™æ€æ•°æ®ï¼Œè¯»å¤šå†™å°‘ | ä¸­ | å¿« |

### 3. è·ç¦»åº¦é‡

```python
# L2ï¼ˆæ¬§æ°è·ç¦»ï¼‰- é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯
index_params_l2 = {
    "metric_type": "L2",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128}
}

# IPï¼ˆå†…ç§¯ï¼‰- é€‚ç”¨äºå½’ä¸€åŒ–å‘é‡
index_params_ip = {
    "metric_type": "IP",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128}
}

# COSINEï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰- é€‚ç”¨äºæ–‡æœ¬å‘é‡
index_params_cosine = {
    "metric_type": "COSINE",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128}
}
```

## å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºè¯­ä¹‰æœç´¢ç³»ç»Ÿ

### å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType
from sentence_transformers import SentenceTransformer
import numpy as np

# 1. è¿æ¥ Milvus
connections.connect(host="localhost", port="19530")

# 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 3. åˆ›å»º Collection
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=1000)
]
schema = CollectionSchema(fields=fields, description="è¯­ä¹‰æœç´¢")
collection = Collection(name="semantic_search", schema=schema)

# 4. å‡†å¤‡æ–‡æ¡£æ•°æ®
documents = [
    "Milvus æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“",
    "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡",
    "æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡",
    "ç›¸ä¼¼åº¦æœç´¢æ˜¯å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒåŠŸèƒ½",
    "Milvus æ”¯æŒå¤šç§ç´¢å¼•ç±»å‹å’Œè·ç¦»åº¦é‡"
]

# 5. ç”Ÿæˆå‘é‡å¹¶æ’å…¥
embeddings = model.encode(documents).tolist()
entities = [embeddings, documents]
collection.insert(entities)
collection.flush()

# 6. åˆ›å»ºç´¢å¼•
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 200}
}
collection.create_index(field_name="embedding", index_params=index_params)
collection.load()

# 7. æ‰§è¡Œè¯­ä¹‰æœç´¢
query = "ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ"
query_embedding = model.encode([query]).tolist()

search_params = {"metric_type": "COSINE", "params": {"ef": 100}}
results = collection.search(
    data=query_embedding,
    anns_field="embedding",
    param=search_params,
    limit=3,
    output_fields=["text"]
)

# 8. è¾“å‡ºç»“æœ
print(f"æŸ¥è¯¢: {query}\n")
for hit in results[0]:
    print(f"ç›¸ä¼¼åº¦: {hit.distance:.4f}")
    print(f"æ–‡æœ¬: {hit.entity.get('text')}\n")
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç´¢å¼•ä¼˜åŒ–
- æ ¹æ®æ•°æ®è§„æ¨¡é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹
- è°ƒæ•´ `nlist` å‚æ•°ï¼ˆIVF ç³»åˆ—ï¼‰ï¼šé€šå¸¸è®¾ç½®ä¸º `sqrt(num_entities)`
- è°ƒæ•´ `nprobe` å‚æ•°ï¼šå¢åŠ å¯æé«˜å¬å›ç‡ä½†é™ä½é€Ÿåº¦

### 2. æŸ¥è¯¢ä¼˜åŒ–
- ä½¿ç”¨åˆ†åŒºå‡å°‘æœç´¢èŒƒå›´
- åˆç†è®¾ç½® `limit` å‚æ•°
- æ‰¹é‡æŸ¥è¯¢ä¼˜äºå•æ¬¡æŸ¥è¯¢

### 3. èµ„æºé…ç½®
- Query Nodeï¼šå¢åŠ å†…å­˜å’Œ CPU æ ¸å¿ƒæ•°
- Data Nodeï¼šå¢åŠ ç£ç›˜ I/O æ€§èƒ½
- ä½¿ç”¨ SSD å­˜å‚¨æå‡æ€§èƒ½

## ç›‘æ§ä¸è¿ç»´

### æŸ¥çœ‹ Collection ä¿¡æ¯

```python
from pymilvus import utility

# åˆ—å‡ºæ‰€æœ‰ Collection
print(utility.list_collections())

# æŸ¥çœ‹ Collection ç»Ÿè®¡ä¿¡æ¯
stats = collection.num_entities
print(f"å®ä½“æ•°é‡: {stats}")

# æŸ¥çœ‹ç´¢å¼•ä¿¡æ¯
print(collection.index().params)
```

### æ•°æ®ç®¡ç†

```python
# åˆ é™¤å®ä½“
expr = "id in [1, 2, 3]"
collection.delete(expr)

# é‡Šæ”¾ Collection
collection.release()

# åˆ é™¤ Collection
collection.drop()
```

## æ€»ç»“

Milvus ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„å‘é‡æ•°æ®åº“ï¼Œä¸º AI åº”ç”¨æä¾›äº†é«˜æ•ˆçš„å‘é‡å­˜å‚¨å’Œæ£€ç´¢èƒ½åŠ›ã€‚é€šè¿‡æœ¬æ–‡çš„ä»‹ç»ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

âœ… ç†è§£ Milvus çš„æ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ  
âœ… å®Œæˆ Milvus çš„å®‰è£…å’Œéƒ¨ç½²  
âœ… æŒæ¡åŸºæœ¬çš„ CRUD æ“ä½œ  
âœ… æ„å»ºå®é™…çš„è¯­ä¹‰æœç´¢åº”ç”¨  
âœ… è¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œè¿ç»´ç®¡ç†

## å‚è€ƒèµ„æº

- [Milvus å®˜æ–¹æ–‡æ¡£](https://milvus.io/docs)
- [Milvus GitHub](https://github.com/milvus-io/milvus)
- [PyMilvus SDK](https://github.com/milvus-io/pymilvus)
- [Milvus Bootcamp](https://github.com/milvus-io/bootcamp)

